{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS498DL Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "noshow"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from kaggle_submission import output_submission_csv\n",
    "from models.neural_net import NeuralNetwork\n",
    "from utils.data_process import get_CIFAR10_data\n",
    "from sklearn.utils import shuffle\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Callable, Dict\n",
    "\n",
    "import os\n",
    "# import time\n",
    "import uuid\n",
    "try:\n",
    "    import dill as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "        \n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)  # set default size of plots\n",
    "\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    def tqdm(x):\n",
    "        return x\n",
    "\n",
    "# For auto-reloading external modules\n",
    "# See http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "noshow"
    ]
   },
   "source": [
    "## Loading CIFAR-10\n",
    "Now that you have implemented a neural network that passes gradient checks and works on toy data, you will test your network on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "noshow"
    ]
   },
   "outputs": [],
   "source": [
    "# You can change these numbers for experimentation\n",
    "# For submission be sure they are set to the default values\n",
    "TRAIN_IMAGES = 49000\n",
    "VAL_IMAGES = 1000\n",
    "TEST_IMAGES = 10000\n",
    "\n",
    "data = get_CIFAR10_data(TRAIN_IMAGES, VAL_IMAGES, TEST_IMAGES)\n",
    "X_train, y_train = data['X_train'], data['y_train']\n",
    "X_val, y_val = data['X_val'], data['y_val']\n",
    "X_test, y_test = data['X_test'], data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "noshow"
    ]
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "noshow"
    ]
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Stats:\n",
    "    '''Class for keeping track of an stats for a trained model.'''\n",
    "    epochs : int\n",
    "    train_loss : np.ndarray = field(init=False)\n",
    "    train_accuracy : np.ndarray = field(init=False)\n",
    "    val_accuracy : np.ndarray = field(init=False)\n",
    "    weight_norms : np.ndarray = field(init=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.train_loss = np.zeros(self.epochs)\n",
    "        self.train_accuracy = np.zeros(self.epochs)\n",
    "        self.val_accuracy = np.zeros(self.epochs)\n",
    "        self.weight_norms = np.zeros(self.epochs)\n",
    "        \n",
    "@dataclass\n",
    "class TrainingParams:\n",
    "    '''Class for keeping track of model params.'''\n",
    "    hidden_size : int\n",
    "    epochs : int\n",
    "    learning_rate : float\n",
    "    learning_rate_decay : float\n",
    "    regularization : float\n",
    "    num_layers : int\n",
    "\n",
    "    # to not be initialized\n",
    "    hidden_sizes : List[int] = field(default_factory=list)\n",
    "    num_classes : int = 10\n",
    "    batch_size : int = 200\n",
    "    input_size : int = 32 * 32 * 3\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        self.hidden_sizes = [self.hidden_size] * (self.num_layers - 1)\n",
    "        \n",
    "@dataclass\n",
    "class TwoLayerTrainingParams(TrainingParams):\n",
    "    num_layers : int = 2\n",
    "        \n",
    "@dataclass\n",
    "class ThreeLayerTrainingParams(TrainingParams):\n",
    "    num_layers : int = 3\n",
    "        \n",
    "class Algorithm:\n",
    "    @dataclass\n",
    "    class AlgorithmImpl:\n",
    "        algorithm : str\n",
    "        \n",
    "    @dataclass\n",
    "    class SGD(AlgorithmImpl):\n",
    "        algorithm : str = \"SGD\"\n",
    "            \n",
    "    @dataclass\n",
    "    class Adam(AlgorithmImpl):\n",
    "        algorithm : str = \"Adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "noshow"
    ]
   },
   "outputs": [],
   "source": [
    "def train_model(params : TrainingParams, algorithm : Algorithm, verbose : bool = True):\n",
    "    # Initialize a new neural network model\n",
    "    net = NeuralNetwork(params.input_size, \n",
    "                        params.hidden_sizes, \n",
    "                        params.num_classes, \n",
    "                        params.num_layers)\n",
    "\n",
    "    stats = Stats(epochs = params.epochs)\n",
    "\n",
    "    f_idx = lambda x : x - 1\n",
    "    def print_stats_at(epoch):\n",
    "        if verbose:\n",
    "            print(\"\".join(('epoch : {}, '.format(epoch),\n",
    "                  'training loss : {}, '.format(stats.train_loss[f_idx(epoch)]),\n",
    "                  'training accuracy: {}, '.format(stats.train_accuracy[f_idx(epoch)]), \n",
    "                  'validation accuracy : {}'.format(stats.val_accuracy[f_idx(epoch)])))\n",
    "                 )\n",
    "\n",
    "    # For each epoch...\n",
    "    learning_rate = params.learning_rate\n",
    "    for epoch in tqdm(range(1, params.epochs + 1)):\n",
    "\n",
    "        # Shuffle the dataset\n",
    "        global X_train, y_train\n",
    "        X_train, y_train = shuffle(X_train, y_train, random_state=None)\n",
    "\n",
    "        # Training\n",
    "        # For each mini-batch...\n",
    "        for batch in range(TRAIN_IMAGES // params.batch_size):\n",
    "            # Create a mini-batch of training data and labels\n",
    "            start_idx = batch * params.batch_size\n",
    "            stop_idx = start_idx + params.batch_size\n",
    "            batch_slice = slice(start_idx, stop_idx, None)\n",
    "\n",
    "            X_batch = X_train[batch_slice, :]\n",
    "            y_batch = y_train[batch_slice]\n",
    "\n",
    "            # Run the forward pass of the model to get a prediction and compute the accuracy\n",
    "            scores = net.forward(X_batch)\n",
    "            prediction = np.argmax(scores, axis=1)\n",
    "            stats.train_accuracy[f_idx(epoch)] += (prediction == y_batch).sum()\n",
    "\n",
    "            # Run the backward pass of the model to compute the loss, and update the weights\n",
    "            stats.train_loss[f_idx(epoch)] += net.backward(y_batch, params.regularization)\n",
    "            net.update(learning_rate, opt = algorithm.algorithm, epoch = epoch)\n",
    "\n",
    "        # normalize at the end\n",
    "        stats.train_accuracy[f_idx(epoch)] /= TRAIN_IMAGES\n",
    "\n",
    "        # Validation\n",
    "        # No need to run the backward pass here, just run the forward pass to compute accuracy\n",
    "        val_scores = net.forward(X_val)\n",
    "        val_prediction = np.argmax(val_scores, axis=1)\n",
    "        # normalize to get ratio\n",
    "        stats.val_accuracy[f_idx(epoch)] += (val_prediction == y_val).sum() / len(y_val)\n",
    "\n",
    "        stats.weight_norms[f_idx(epoch)] = net.weight_norms()\n",
    "        \n",
    "        if not (epoch) % 5:\n",
    "            print_stats_at(epoch)\n",
    "\n",
    "        # Implement learning rate decay\n",
    "        learning_rate *= params.learning_rate_decay\n",
    "\n",
    "    # print at final time\n",
    "    print_stats_at(epoch)\n",
    "    \n",
    "    return net, stats\n",
    "\n",
    "def plot_stats(some_stats):\n",
    "    try:\n",
    "        import plotly.graph_objects as go\n",
    "        from plotly.subplots import make_subplots\n",
    "\n",
    "        # fig = go.Figure()\n",
    "        fig = make_subplots(rows=3, \n",
    "                            cols=1, \n",
    "                            subplot_titles=(\"Weight norm history\", \"Loss history\", \"Accuracy history\"))\n",
    "        # l2 norm\n",
    "        fig.add_trace(go.Scatter(y=some_stats.weight_norms,\n",
    "                            mode='lines+markers', name=\"W norm\"), row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"W norm\", row=1, col=1)\n",
    "\n",
    "        # losses\n",
    "        fig.add_trace(go.Scatter(y = some_stats.train_loss,\n",
    "                            mode='lines+markers',\n",
    "                            name='training_loss'), row=2, col=1)\n",
    "        # Update xaxis properties\n",
    "        fig.update_yaxes(title_text=\"Loss\", row=2, col=1)\n",
    "\n",
    "        # losses\n",
    "        fig.add_trace(go.Scatter(y = some_stats.train_accuracy,\n",
    "                            mode='lines+markers',\n",
    "                            name='train_accuracy'), row=3, col=1)\n",
    "        fig.add_trace(go.Scatter(y = some_stats.val_accuracy,\n",
    "                            mode='lines+markers',\n",
    "                            name='val_accuracy'), row=3, col=1)\n",
    "        # Update xaxis properties\n",
    "        fig.update_yaxes(title_text=\"Accuracy\", row=3, col=1)\n",
    "        fig.update_xaxes(title_text=\"epochs\", row=3, col=1)\n",
    "\n",
    "        fig.show(renderer=\"notebook+pdf\")\n",
    "        return fig\n",
    "    \n",
    "    except ImportError:\n",
    "        # Plot the loss function and train / validation accuracies\n",
    "        plt.subplot(3, 1, 1)\n",
    "        plt.plot(some_stats.weight_norms)\n",
    "        plt.title('Weight norm history')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Frobenius norm of weights')\n",
    "\n",
    "        plt.subplot(3, 1, 2)\n",
    "        plt.plot(some_stats.train_loss)\n",
    "        plt.title('Loss history')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "\n",
    "        plt.subplot(3, 1, 3)\n",
    "        plt.plot(some_stats.train_accuracy, label='train')\n",
    "        plt.plot(some_stats.val_accuracy, label='val')\n",
    "        plt.title('Classification accuracy history')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Classification accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def report_test_accuracy(trained_network : NeuralNetwork):\n",
    "    global X_test, y_test\n",
    "    test_scores = trained_network.forward(X_test)\n",
    "    test_prediction = np.argmax(test_scores, axis=1)\n",
    "    return test_prediction, (test_prediction == y_test).sum() / len(y_test)\n",
    "        \n",
    "def generate_report(trained_network : NeuralNetwork, \n",
    "                    training_stats : Stats, \n",
    "                    verbose : bool = True):\n",
    "    test_prediction, acc = report_test_accuracy(trained_network)\n",
    "    if verbose:\n",
    "        print(\"Test accuracy obtained is \", acc)\n",
    "        plot_stats(training_stats)\n",
    "    return test_prediction, acc\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    def __init__(self, params, algorithm_cls):       \n",
    "        self.checkpoint_path = os.path.join(os.getcwd(), \n",
    "                                            type(params).__name__ + \"_\" + algorithm_cls.__name__) \n",
    "        os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "    \n",
    "    def save(self, nn, stats, clean_old = False):\n",
    "        if clean_old:\n",
    "            files = filter(lambda x: x.endswith('.pkl'), os.listdir(self.checkpoint_path))\n",
    "            files = [os.path.join(self.checkpoint_path, f) for f in files] # add path to each file\n",
    "            [os.remove(f) for f in files]\n",
    "        \n",
    "        file_name = str(uuid.uuid4())\n",
    "        with open(os.path.join(self.checkpoint_path , file_name + '.pkl'), 'wb') as f:\n",
    "            pickle.dump(nn, f)\n",
    "        with open(os.path.join(self.checkpoint_path , file_name + '.stats_pkl'), 'wb') as f:\n",
    "            pickle.dump(stats, f)\n",
    "    \n",
    "    def load_latest(self):\n",
    "        files = filter(lambda x: x.endswith('.pkl'), os.listdir(self.checkpoint_path))\n",
    "        files = [os.path.join(self.checkpoint_path, f) for f in files] # add path to each file\n",
    "        files.sort(key=lambda x: os.path.getmtime(x))\n",
    "        file_name = files[-1]\n",
    "        with open(os.path.join(self.checkpoint_path , file_name), 'rb') as f:\n",
    "            nn = pickle.load(f)\n",
    "\n",
    "        stat_files = filter(lambda x: x.endswith('.stats_pkl'), os.listdir(self.checkpoint_path))\n",
    "        stat_files = [os.path.join(self.checkpoint_path, f) for f in stat_files] # add path to each file\n",
    "        stat_files.sort(key=lambda x: os.path.getmtime(x))\n",
    "        stat_file_name = stat_files[-1]\n",
    "    \n",
    "        def file_name_without_ext(fn):\n",
    "            temp = os.path.splitext(fn)\n",
    "            var = os.path.basename(temp[0])\n",
    "            return var\n",
    "        \n",
    "        assert file_name_without_ext(stat_file_name) == file_name_without_ext(file_name)\n",
    "        \n",
    "        with open(os.path.join(self.checkpoint_path , stat_file_name), 'rb') as f:\n",
    "            stats = pickle.load(f)\n",
    "\n",
    "        return nn, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "noshow"
    ]
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 32 * 32 * 3\n",
    "\n",
    "\"\"\"\n",
    "# Given by the CS498 junta\n",
    "num_layers = 2\n",
    "hidden_size = 20\n",
    "hidden_sizes = [hidden_size] * (num_layers - 1)\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "batch_size = 200\n",
    "learning_rate = 1e-3\n",
    "learning_rate_decay = 0.95\n",
    "regularization = 0.1\n",
    "\"\"\"\n",
    "\n",
    "optimization_algorithm = \"SGD\"\n",
    "# optimization_algorithm = \"Adam\"\n",
    "\n",
    "if optimization_algorithm == \"SGD\":\n",
    "    \"\"\"\n",
    "    num_layers = 2\n",
    "    hidden_size = 80, # even 100 is the same\n",
    "    hidden_sizes = [hidden_size] * (num_layers - 1)\n",
    "    num_classes = 10\n",
    "    epochs = 30\n",
    "    batch_size = 200\n",
    "    learning_rate = 1e-2\n",
    "    learning_rate_decay = 0.99\n",
    "    regularization = 0.03\n",
    "    # epoch : 29, training loss : 394.92802593721126, training accuracy: 0.5258163265306123, validation accuracy : 0.503\n",
    "\n",
    "    num_layers = 2\n",
    "    hidden_size = 100\n",
    "    hidden_sizes = [hidden_size] * (num_layers - 1)\n",
    "    num_classes = 10\n",
    "    epochs = 30\n",
    "    batch_size = 200\n",
    "    learning_rate = 1e-2\n",
    "    learning_rate_decay = 0.99\n",
    "    regularization = 0.01    \n",
    "    epoch : 30, training loss : 368.79332463881946, training accuracy: 0.5802857142857143, validation accuracy : 0.531\n",
    "    \"\"\"\n",
    "    num_layers = 2\n",
    "    hidden_size = 100\n",
    "    hidden_sizes = [hidden_size] * (num_layers - 1)\n",
    "    num_classes = 10\n",
    "    epochs = 30\n",
    "    batch_size = 200\n",
    "    learning_rate = 1e-2\n",
    "    learning_rate_decay = 0.99\n",
    "    regularization = 0.01    \n",
    "elif optimization_algorithm == \"Adam\":\n",
    "    \"\"\"\n",
    "    num_layers = 2\n",
    "    hidden_size = 80\n",
    "    hidden_sizes = [hidden_size] * (num_layers - 1)\n",
    "    num_classes = 10\n",
    "    epochs = 20\n",
    "    batch_size = 200\n",
    "    learning_rate = 1e-2\n",
    "    learning_rate_decay = 0.95\n",
    "    regularization = 0.005\n",
    "    epoch : 20, training loss : 345.92703000778187, training accuracy: 0.556795918367347, validation accuracy : 0.524\n",
    "\n",
    "\n",
    "    num_layers = 2\n",
    "    hidden_size = 100\n",
    "    hidden_sizes = [hidden_size] * (num_layers - 1)\n",
    "    num_classes = 10\n",
    "    epochs = 20\n",
    "    batch_size = 200\n",
    "    learning_rate = 1e-2\n",
    "    learning_rate_decay = 0.95\n",
    "    regularization = 0.002\n",
    "    epoch : 20, training loss : 311.3749559459592, training accuracy: 0.6064693877551021, validation accuracy : 0.54\n",
    "\n",
    "    num_layers = 2\n",
    "    hidden_size = 100\n",
    "    hidden_sizes = [hidden_size] * (num_layers - 1)\n",
    "    num_classes = 10\n",
    "    epochs = 30\n",
    "    batch_size = 200\n",
    "    learning_rate = 1e-2\n",
    "    learning_rate_decay = 0.95\n",
    "    regularization = 0.005\n",
    "    epoch : 30, training loss : 324.4904180432253, training accuracy: 0.5893061224489796, validation accuracy : 0.55    \n",
    "    \"\"\"\n",
    "    num_layers = 2\n",
    "    hidden_size = 100\n",
    "    hidden_sizes = [hidden_size] * (num_layers - 1)\n",
    "    num_classes = 10\n",
    "    epochs = 20\n",
    "    batch_size = 200\n",
    "    learning_rate = 1e-2\n",
    "    learning_rate_decay = 0.95\n",
    "    regularization = 0.005\n",
    "\n",
    "# Initialize a new neural network model\n",
    "net = NeuralNetwork(input_size, hidden_sizes, num_classes, num_layers)\n",
    "\n",
    "# Variables to store performance for each epoch\n",
    "train_loss = np.zeros(epochs)\n",
    "train_accuracy = np.zeros(epochs)\n",
    "val_accuracy = np.zeros(epochs)\n",
    "\n",
    "f_idx = lambda x : x - 1\n",
    "def print_stats_at(epoch):\n",
    "    print(\"\".join(('epoch : {}, '.format(epoch),\n",
    "          'training loss : {}, '.format(train_loss[f_idx(epoch)]),\n",
    "          'training accuracy: {}, '.format(train_accuracy[f_idx(epoch)]), \n",
    "          'validation accuracy : {}'.format(val_accuracy[f_idx(epoch)])))\n",
    "         )\n",
    "\n",
    "# For each epoch...\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=None)\n",
    "\n",
    "    # Training\n",
    "    # For each mini-batch...\n",
    "    for batch in range(TRAIN_IMAGES // batch_size):\n",
    "        # Create a mini-batch of training data and labels\n",
    "        start_idx = batch * batch_size\n",
    "        stop_idx = start_idx + batch_size\n",
    "        batch_slice = slice(start_idx, stop_idx, None)\n",
    "\n",
    "        X_batch = X_train[batch_slice, :]\n",
    "        y_batch = y_train[batch_slice]\n",
    "        # print(y_batch.shape)\n",
    "        \n",
    "        # Run the forward pass of the model to get a prediction and compute the accuracy\n",
    "        scores = net.forward(X_batch)\n",
    "        prediction = np.argmax(scores, axis=1)\n",
    "        train_accuracy[f_idx(epoch)] += (prediction == y_batch).sum()\n",
    "\n",
    "        # Run the backward pass of the model to compute the loss, and update the weights\n",
    "        train_loss[f_idx(epoch)] += net.backward(y_batch, regularization)\n",
    "        net.update(learning_rate, opt = optimization_algorithm, epoch = epoch)\n",
    "\n",
    "    # normalize at the end\n",
    "    train_accuracy[f_idx(epoch)] /= TRAIN_IMAGES\n",
    "    \n",
    "    # Validation\n",
    "    # No need to run the backward pass here, just run the forward pass to compute accuracy\n",
    "    val_scores = net.forward(X_val)\n",
    "    val_prediction = np.argmax(val_scores, axis=1)\n",
    "    # normalize to get ratio\n",
    "    val_accuracy[f_idx(epoch)] += (val_prediction == y_val).sum() / len(y_val)\n",
    "    \n",
    "    if not (epoch) % 5:\n",
    "        print_stats_at(epoch)\n",
    "    \n",
    "    # Implement learning rate decay\n",
    "    learning_rate *= learning_rate_decay\n",
    "\n",
    "# print at final time\n",
    "print_stats_at(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "noshow"
    ]
   },
   "source": [
    "## Graph loss and train/val accuracies\n",
    "\n",
    "Examining the loss graph along with the train and val accuracy graphs should help you gain some intuition for the hyperparameters you should try in the hyperparameter tuning below. It should also help with debugging any issues you might have with your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "noshow"
    ]
   },
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_loss)\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_accuracy, label='train')\n",
    "plt.plot(val_accuracy, label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "We only show the relevant parts of our results (training reports and plots) and leave out the details of implementation. The implementation can be found in the attached notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using SGD\n",
    "To train our network we will use SGD. In addition, we will adjust the learning rate with an exponential learning rate schedule as optimization proceeds; after each epoch, we will reduce the learning rate by multiplying it by a decay rate.\n",
    "\n",
    "You can try different numbers of layers and other hyperparameters on the CIFAR-10 dataset below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two layer training with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Best Coarse search results, fine search results not included, see section on Hyperparameter tuning below\n",
    "1\n",
    "epoch : 30, training loss : 370.39517725296315, training accuracy: 0.5788775510204082, validation accuracy : 0.527\n",
    "\n",
    "{'hidden_size': 100,\n",
    " 'epochs': 30,\n",
    " 'learning_rate': 0.01,\n",
    " 'learning_rate_decay': 0.99,\n",
    " 'regularization': 0.01,\n",
    " 'num_layers': 2,\n",
    " 'hidden_sizes': [100],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072}\n",
    " \n",
    "2\n",
    "More or less saturated\n",
    "epoch : 30, training loss : 366.56928722194294, training accuracy: 0.5678367346938775, validation accuracy : 0.543\n",
    "\n",
    "{'hidden_size': 80,\n",
    " 'epochs': 30,\n",
    " 'learning_rate': 0.01,\n",
    " 'learning_rate_decay': 0.99,\n",
    " 'regularization': 0.01,\n",
    " 'num_layers': 2,\n",
    " 'hidden_sizes': [80],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072}\n",
    " \n",
    "3\n",
    "Saturated\n",
    "epoch : 30, training loss : 365.5431457113199, training accuracy: 0.5580408163265306, validation accuracy : 0.521\n",
    "\n",
    "{'hidden_size': 60,\n",
    " 'epochs': 30,\n",
    " 'learning_rate': 0.01,\n",
    " 'learning_rate_decay': 0.99,\n",
    " 'regularization': 0.01,\n",
    " 'num_layers': 2,\n",
    " 'hidden_sizes': [60],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072}\n",
    "\n",
    "4\n",
    "Saturates very quickly upon increasing learnign rate\n",
    "epoch : 20, training loss : 340.3045509467915, training accuracy: 0.5846326530612245, validation accuracy : 0.514\n",
    "\n",
    "{'hidden_size': 80,\n",
    " 'epochs': 20,\n",
    " 'learning_rate': 0.05,\n",
    " 'learning_rate_decay': 0.99,\n",
    " 'regularization': 0.01,\n",
    " 'num_layers': 2,\n",
    " 'hidden_sizes': [80],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072}\n",
    " \n",
    "5\n",
    "For a low learning rate, it saturates late, but the training accuracy is somewhat lesser\n",
    "epoch : 30, training loss : 407.2636209485998, training accuracy: 0.5327142857142857, validation accuracy : 0.506\n",
    "\n",
    "{'hidden_size': 80,\n",
    " 'epochs': 30,\n",
    " 'learning_rate': 0.005,\n",
    " 'learning_rate_decay': 0.99,\n",
    " 'regularization': 0.01,\n",
    " 'num_layers': 2,\n",
    " 'hidden_sizes': [80],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072}\n",
    "\n",
    "6\n",
    "For a slightly lower decay, things are more or less the same.\n",
    "However for a devay rate of 0.9 things converge quickly and give bad results.\n",
    "epoch : 30, training loss : 392.1978567049139, training accuracy: 0.5486734693877551, validation accuracy : 0.51\n",
    "\n",
    "{'hidden_size': 80,\n",
    " 'epochs': 30,\n",
    " 'learning_rate': 0.01,\n",
    " 'learning_rate_decay': 0.95,\n",
    " 'regularization': 0.01,\n",
    " 'num_layers': 2,\n",
    " 'hidden_sizes': [80],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072}\n",
    "\n",
    "epoch : 30, training loss : 422.5223690800481, training accuracy: 0.5222653061224489, validation accuracy : 0.484\n",
    "\n",
    "{'hidden_size': 80,\n",
    " 'epochs': 30,\n",
    " 'learning_rate': 0.01,\n",
    " 'learning_rate_decay': 0.9,\n",
    " 'regularization': 0.01,\n",
    " 'num_layers': 2,\n",
    " 'hidden_sizes': [80],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072}\n",
    " \n",
    "7\n",
    "For lower values of regularization coefficient (0.001, 0.005), there is definitely overfitting \n",
    "epoch : 30, training loss : 307.4043522173789, training accuracy: 0.588, validation accuracy : 0.51\n",
    "\n",
    "{'hidden_size': 80,\n",
    " 'epochs': 30,\n",
    " 'learning_rate': 0.01,\n",
    " 'learning_rate_decay': 0.99,\n",
    " 'regularization': 0.001,\n",
    " 'num_layers': 2,\n",
    " 'hidden_sizes': [80],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072} \n",
    " \n",
    "8\n",
    "For higher reg (0.02, 0.04) things either take longer to converge with a moderate learnign rate\n",
    "or converge at a slightly smaller accuracy\n",
    "epoch : 30, training loss : 383.13177333741055, training accuracy: 0.5468571428571428, validation accuracy : 0.511\n",
    "\n",
    "{'hidden_size': 80,\n",
    " 'epochs': 30,\n",
    " 'learning_rate': 0.01,\n",
    " 'learning_rate_decay': 0.99,\n",
    " 'regularization': 0.02,\n",
    " 'num_layers': 2,\n",
    " 'hidden_sizes': [80],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072}\n",
    " \n",
    "Most Optimal Found:\n",
    "epoch : 30, training loss : 365.30765684235206, training accuracy: 0.5643469387755102, validation accuracy : 0.522\n",
    "\n",
    "{'hidden_size': 80,\n",
    " 'epochs': 30,\n",
    " 'learning_rate': 0.02,\n",
    " 'learning_rate_decay': 0.99,\n",
    " 'regularization': 0.02,\n",
    " 'num_layers': 2,\n",
    " 'hidden_sizes': [80],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072}\n",
    " \n",
    " epoch : 30, training loss : 367.225473479852, training accuracy: 0.5606326530612245, validation accuracy : 0.536\n",
    "\n",
    "{'hidden_size': 80,\n",
    " 'epochs': 30,\n",
    " 'learning_rate': 0.05,\n",
    " 'learning_rate_decay': 0.99,\n",
    " 'regularization': 0.02,\n",
    " 'num_layers': 2,\n",
    " 'hidden_sizes': [80],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072}\n",
    "\n",
    "Based on these results, we vary parameters as \n",
    "hidden_size = [80, 100, 120]\n",
    "epochs = 30\n",
    "learning_rate = [1e-2, 2e-2, 5e-2, (8e-2)]\n",
    "learning_rate_decay = [0.95, 0.99]\n",
    "regularization = [0.005, 0.01, 0.02, (0.04)]\n",
    "\"\"\"\n",
    "\n",
    "# These are the best parameter sets found after running a fine search of the phase space\n",
    "minimal_optimal_params = {\n",
    "    \"hidden_size\": 120,\n",
    "    \"epochs\": 30,\n",
    "    \"learning_rate\": 8e-2,\n",
    "    \"learning_rate_decay\": 0.95,\n",
    "    \"regularization\": 0.01,\n",
    "}\n",
    "\n",
    "params = TwoLayerTrainingParams(**minimal_optimal_params)\n",
    "if False:\n",
    "    # Train a new model\n",
    "    trained_net, stats = train_model(params, Algorithm.SGD)\n",
    "else:\n",
    "    # Load model from previously trained, use for final output and kaggle submission\n",
    "    trained_net, stats = ModelCheckpoint(params, Algorithm.SGD).load_latest()\n",
    "asdict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_2layer_sgd_prediction, best_2layer_sgd_accuracy = generate_report(trained_net, stats)\n",
    "if False:\n",
    "    output_submission_csv('./nn_2layer_sgd_submission.csv', best_2layer_sgd_prediction)\n",
    "    ModelCheckpoint(params, Algorithm.SGD).save(trained_net, stats, clean_old=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three layer training with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Best Coarse search results, fine search results not included, see section on Hyperparameter tuning below\n",
    " \n",
    "Seems good\n",
    "epoch : 30, training loss : 385.3684869757984, training accuracy: 0.5543673469387755, validation accuracy : 0.523\n",
    "\n",
    "{'hidden_size': 100,\n",
    " 'epochs': 30,\n",
    " 'learning_rate': 0.05,\n",
    " 'learning_rate_decay': 0.99,\n",
    " 'regularization': 0.02,\n",
    " 'num_layers': 3,\n",
    " 'hidden_sizes': [100, 100],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072}\n",
    " \n",
    "A slightly higher learning rate but paired with a faster decay also helps\n",
    "epoch : 30, training loss : 375.8179540220022, training accuracy: 0.5774081632653061, validation accuracy : 0.537\n",
    "\n",
    "{'hidden_size': 100,\n",
    " 'epochs': 30,\n",
    " 'learning_rate': 0.08,\n",
    " 'learning_rate_decay': 0.95,\n",
    " 'regularization': 0.02,\n",
    " 'num_layers': 3,\n",
    " 'hidden_sizes': [100, 100],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072}\n",
    " \n",
    " 2. Starts overfitting like crazy for slighly smaller lambdas too\n",
    "epoch : 30, training loss : 307.3058261118782, training accuracy: 0.6529591836734694, validation accuracy : 0.532\n",
    "\n",
    "{'hidden_size': 100,\n",
    " 'epochs': 30,\n",
    " 'learning_rate': 0.05,\n",
    " 'learning_rate_decay': 0.99,\n",
    " 'regularization': 0.005,\n",
    " 'num_layers': 3,\n",
    " 'hidden_sizes': [100, 100],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072}\n",
    " \n",
    " Meanwhile for higher lambdas, it doesnt budge much\n",
    " epoch : 30, training loss : 450.69301639983144, training accuracy: 0.47044897959183674, validation accuracy : 0.464\n",
    "\n",
    "{'hidden_size': 100,\n",
    " 'epochs': 30,\n",
    " 'learning_rate': 0.05,\n",
    " 'learning_rate_decay': 0.99,\n",
    " 'regularization': 0.05,\n",
    " 'num_layers': 3,\n",
    " 'hidden_sizes': [100, 100],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072}\n",
    " \n",
    " \n",
    "We sweep the same parameters in this case.\n",
    "\"\"\"\n",
    "# These are the best parameter sets found after running a fine search of the phase space\n",
    "# In this case, two parameter sets gave good, equivalent results\n",
    "minimal_optimal_params_candidate_1 = {\n",
    "    \"hidden_size\": 100,\n",
    "    \"epochs\": 25,\n",
    "    \"learning_rate\": 8e-2,\n",
    "    \"learning_rate_decay\": 0.95,\n",
    "    \"regularization\": 0.01,\n",
    "}\n",
    "\n",
    "minimal_optimal_params_candidate_2 = {\n",
    "    \"hidden_size\": 120,\n",
    "    \"epochs\": 30,\n",
    "    \"learning_rate\": 5e-2,\n",
    "    \"learning_rate_decay\": 0.95,\n",
    "    \"regularization\": 0.01,\n",
    "}\n",
    "\n",
    "params = ThreeLayerTrainingParams(**minimal_optimal_params_candidate_2)\n",
    "\n",
    "if False:\n",
    "    # Train a new model\n",
    "    trained_net, stats = train_model(params, Algorithm.SGD)\n",
    "else:\n",
    "    # Load model from previously trained, use for final output and kaggle submission\n",
    "    trained_net, stats = ModelCheckpoint(params, Algorithm.SGD).load_latest()\n",
    "asdict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_3layer_sgd_prediction, best_3layer_sgd_accuracy = generate_report(trained_net, stats)\n",
    "if False:\n",
    "    output_submission_csv('./nn_3layer_sgd_submission.csv', best_3layer_sgd_prediction)\n",
    "    ModelCheckpoint(params, Algorithm.SGD).save(trained_net, stats, clean_old=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using Adam\n",
    "Next we will train the same model using the Adam optimizer. You should take the above code for SGD and modify it to use Adam instead. For implementation details, see the lecture slides. The original paper that introduced Adam is also a good reference, and contains suggestions for default values: https://arxiv.org/pdf/1412.6980.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two layer training with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Best Coarse search results, fine search results not included, see section on Hyperparameter tuning below\n",
    "1.\n",
    "weight norm has an interesting trend where it first drops and then increases back again\n",
    "\n",
    "epoch : 30, training loss : 323.03653688232475, training accuracy: 0.5964489795918367, validation accuracy : 0.547\n",
    "\n",
    "{'hidden_size': 100,\n",
    " 'epochs': 30,\n",
    " 'learning_rate': 0.01,\n",
    " 'learning_rate_decay': 0.95,\n",
    " 'regularization': 0.005,\n",
    " 'num_layers': 2,\n",
    " 'hidden_sizes': [100],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072}\n",
    " \n",
    "2.\n",
    "Increasing regularization slightly worsens performane, but not alarmingly.\n",
    "Decresing regularization overfits at the same level of performance\n",
    "epoch : 30, training loss : 351.9264538779735, training accuracy: 0.5538775510204081, validation accuracy : 0.53\n",
    "\n",
    "{'hidden_size': 100,\n",
    " 'epochs': 30,\n",
    " 'learning_rate': 0.01,\n",
    " 'learning_rate_decay': 0.95,\n",
    " 'regularization': 0.01,\n",
    " 'num_layers': 2,\n",
    " 'hidden_sizes': [100],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072}\n",
    " \n",
    " epoch : 30, training loss : 282.7108224406282, training accuracy: 0.6563469387755102, validation accuracy : 0.539\n",
    "\n",
    "{'hidden_size': 100,\n",
    " 'epochs': 30,\n",
    " 'learning_rate': 0.01,\n",
    " 'learning_rate_decay': 0.95,\n",
    " 'regularization': 0.002,\n",
    " 'num_layers': 2,\n",
    " 'hidden_sizes': [100],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072}\n",
    "\n",
    "3.\n",
    "Didnt reach convergence yer, even with a higher learning rate\n",
    "epoch : 30, training loss : 377.2422995167926, training accuracy: 0.5025714285714286, validation accuracy : 0.501\n",
    "\n",
    "{'hidden_size': 100,\n",
    " 'epochs': 30,\n",
    " 'learning_rate': 0.02,\n",
    " 'learning_rate_decay': 0.95,\n",
    " 'regularization': 0.01,\n",
    " 'num_layers': 2,\n",
    " 'hidden_sizes': [100],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072}\n",
    "\n",
    "increased learning rate with increased decays also performs same more or less\n",
    "epoch : 30, training loss : 313.03113190638817, training accuracy: 0.6172244897959184, validation accuracy : 0.541\n",
    "\n",
    "{'hidden_size': 100,\n",
    " 'epochs': 30,\n",
    " 'learning_rate': 0.05,\n",
    " 'learning_rate_decay': 0.9,\n",
    " 'regularization': 0.002,\n",
    " 'num_layers': 2,\n",
    " 'hidden_sizes': [100],\n",
    " 'num_classes': 10,\n",
    " 'batch_size': 200,\n",
    " 'input_size': 3072}\n",
    "\n",
    "We sweep the same parameters as SGD in this case.\n",
    "\"\"\"\n",
    "    \n",
    "# These are the best parameter sets found after running a fine search of the phase space\n",
    "minimal_optimal_params = {\n",
    "    \"hidden_size\": 100,\n",
    "    \"epochs\": 30,\n",
    "    \"learning_rate\": 1e-2,\n",
    "    \"learning_rate_decay\": 0.95,\n",
    "    \"regularization\": 0.005,\n",
    "}\n",
    "params = TwoLayerTrainingParams(**minimal_optimal_params)\n",
    "if False:\n",
    "    # Train a new model\n",
    "    trained_net, stats = train_model(params, Algorithm.Adam)\n",
    "else:\n",
    "    # Load model from previously trained, use for final output and kaggle submission\n",
    "    trained_net, stats = ModelCheckpoint(params, Algorithm.Adam).load_latest()\n",
    "asdict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_2layer_adam_prediction, best_2layer_adam_accuracy = generate_report(trained_net, stats)\n",
    "if False:\n",
    "    output_submission_csv('./nn_2layer_adam_submission.csv', best_2layer_adam_prediction)\n",
    "    ModelCheckpoint(params, Algorithm.Adam).save(trained_net, stats, clean_old=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three layer training with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Best Coarse search results, fine search results not included, see section on Hyperparameter tuning below\n",
    "\n",
    "We sweep the same parameters as SGD in this case.\n",
    "\"\"\"\n",
    "minimal_optimal_params_candidate_1 = {\n",
    "    \"hidden_size\": 80,\n",
    "    \"epochs\": 40,\n",
    "    \"learning_rate\": 1e-2,\n",
    "    \"learning_rate_decay\": 0.95,\n",
    "    \"regularization\": 0.005,\n",
    "}\n",
    "\n",
    "minimal_optimal_params_candidate_2 = {\n",
    "    \"hidden_size\": 100,\n",
    "    \"epochs\": 40,\n",
    "    \"learning_rate\": 1e-2,\n",
    "    \"learning_rate_decay\": 0.95,\n",
    "    \"regularization\": 0.005,\n",
    "}\n",
    "\n",
    "# These are the best parameter sets found after running a fine search of the phase space\n",
    "params = ThreeLayerTrainingParams(**minimal_optimal_params_candidate_1)\n",
    "if False:\n",
    "    # Train a new model\n",
    "    trained_net, stats = train_model(params, Algorithm.Adam)\n",
    "else:\n",
    "    # Load model from previously trained, use for final output and kaggle submission\n",
    "    trained_net, stats = ModelCheckpoint(params, Algorithm.Adam).load_latest()\n",
    "asdict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_3layer_adam_prediction, best_3layer_adam_accuracy = generate_report(trained_net, stats)\n",
    "if True:\n",
    "    output_submission_csv('./nn_3layer_adam_submission.csv', best_3layer_adam_prediction)\n",
    "    ModelCheckpoint(params, Algorithm.Adam).save(trained_net, stats, clean_old=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "noshow"
    ]
   },
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "Once you have successfully trained a network you can tune your hyparameters to increase your accuracy.\n",
    "\n",
    "Based on the graphs of the loss function above you should be able to develop some intuition about what hyperparameter adjustments may be necessary. A very noisy loss implies that the learning rate might be too high, while a linearly decreasing loss would suggest that the learning rate may be too low. A large gap between training and validation accuracy would suggest overfitting due to a large model without much regularization. No gap between training and validation accuracy would indicate low model capacity. \n",
    "\n",
    "You will compare networks of two and three layers using the different optimization methods you implemented. \n",
    "\n",
    "The different hyperparameters you can experiment with are:\n",
    "- **Batch size**: We recommend you leave this at 200 initially which is the batch size we used. \n",
    "- **Number of iterations**: You can gain an intuition for how many iterations to run by checking when the validation accuracy plateaus in your train/val accuracy graph.\n",
    "- **Initialization** Weight initialization is very important for neural networks. We used the initialization `W = np.random.randn(n) / sqrt(n)` where `n` is the input dimension for layer corresponding to `W`. We recommend you stick with the given initializations, but you may explore modifying these. Typical initialization practices: http://cs231n.github.io/neural-networks-2/#init\n",
    "- **Learning rate**: Generally from around 1e-4 to 1e-1 is a good range to explore according to our implementation.\n",
    "- **Learning rate decay**: We recommend a 0.95 decay to start.\n",
    "- **Hidden layer size**: You should explore up to around 120 units per layer. For three-layer network, we fixed the two hidden layers to be the same size when obtaining the target numbers. However, you may experiment with having different size hidden layers.\n",
    "- **Regularization coefficient**: We recommend trying values in the range 0 to 0.1. \n",
    "\n",
    "Hints:\n",
    "- After getting a sense of the parameters by trying a few values yourself, you will likely want to write a few for-loops to traverse over a set of hyperparameters.\n",
    "- If you find that your train loss is decreasing, but your train and val accuracy start to decrease rather than increase, your model likely started minimizing the regularization term. To prevent this you will need to decrease the regularization coefficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "In the output shown above, in each cell, we detail the hyperparameters generated by a coarse, first-pass search. We then carry out a finer sweep of the hyperparameter space. The code for implementing this parameter sweep is not shown in the output for the sake of brevity, but can be found in the attached notebook. We only show the client (with its results cleared, for brevity) and how to call the sweep. The higher-level details of parameter search can be found in the report instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "noshow"
    ]
   },
   "outputs": [],
   "source": [
    "import psweep as ps\n",
    "import pandas as pd\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def get_calc_dir(num_layers: int, algorithm_cls: Algorithm):\n",
    "    return \"calc_\" + algorithm_cls.__name__ + \"_{}layer\".format(num_layers)\n",
    "\n",
    "def run_campaign(params_cls, algorithm_cls, raw_parameters : List[Dict]):\n",
    "    pbar = tqdm(total = len(raw_parameters), position = 1)\n",
    "    \n",
    "    def _run_training(pset):\n",
    "        params = params_cls(hidden_size = pset['hidden_size'], \n",
    "                            epochs = pset['epochs'], \n",
    "                            learning_rate = pset['learning_rate'], \n",
    "                            learning_rate_decay = pset['learning_rate_decay'], \n",
    "                            regularization = pset['regularization'])\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # dont ensemble\n",
    "        total_ensembles = 1\n",
    "        stats_ensemble_history = [None for _ in range(total_ensembles)]\n",
    "        test_accuracy_history = []\n",
    "        for trial in range(total_ensembles):\n",
    "            trained_net, stats = train_model(params, algorithm_cls, verbose = False)\n",
    "            stats_ensemble_history[trial] = stats\n",
    "            # _, test_accuracy[trial] = generate_report(trained_net, stats, verbose=False)\n",
    "        \n",
    "        return dict({'stats_history' : stats_ensemble_history}, **asdict(params))\n",
    "    \n",
    "    return ps.run(_run_training, raw_parameters, calc_dir = get_calc_dir(params_cls.num_layers, algorithm_cls), simulate=False)\n",
    "\n",
    "# should not be none though\n",
    "def report_results_of(\n",
    "    run_df: Union[pd.DataFrame, None],\n",
    "    params_cls: TrainingParams,\n",
    "    algorithm_cls: Algorithm,\n",
    "):\n",
    "    if not isinstance(run_df, pd.DataFrame):\n",
    "        import os\n",
    "\n",
    "        run_df = ps.df_read(\n",
    "            os.path.join(\n",
    "                get_calc_dir(params_cls.num_layers, algorithm_cls), 'results.pk'\n",
    "            )\n",
    "        )\n",
    "\n",
    "    \"\"\"\n",
    "    best_max_loc = None\n",
    "    best_last_loc = None\n",
    "    best_max = -1.0\n",
    "    best_last = -1.0\n",
    "\n",
    "    for loc_stat_h, stat_h in enumerate(run_df.stats_history):\n",
    "        loc_max = -1.0\n",
    "        loc_last = -1.0\n",
    "        for trials in stat_h:\n",
    "            loc_max = max(loc_max, trials.val_accuracy.max())\n",
    "            loc_last = max(loc_last, trials.val_accuracy[-1])\n",
    "        if loc_max >= best_max:\n",
    "            best_max_loc = loc_stat_h\n",
    "            best_max = loc_max\n",
    "        if loc_last >= best_last:\n",
    "            best_last_loc = loc_stat_h\n",
    "            best_last = loc_last\n",
    "\n",
    "    print(\"Best max validation accuracy :\", best_max, \" Best last validation accuracy :\", best_last)\n",
    "    print(\"Best max validation accuracy index :\", best_max_loc, \" Best last validation accuracy index :\", best_last_loc)\n",
    "    \"\"\"\n",
    "    # assume single trial\n",
    "    last_accuracies = np.array([trial.val_accuracy[-1] for stat in run_df.stats_history for trial in stat])\n",
    "    max_accuracies = np.array([trial.val_accuracy.max() for stat in run_df.stats_history for trial in stat])\n",
    "    \n",
    "    # Plot the loss function and train / validation accuracies\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(last_accuracies)\n",
    "    plt.title('last_acc')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Acc')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(max_accuracies)\n",
    "    plt.title('max acc')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    threshold = 0.52\n",
    "    last_indices = np.where(last_accuracies > threshold)\n",
    "    max_indices = np.where(max_accuracies > threshold)\n",
    "    print(\"Best max validation accuracy :\", max_accuracies[max_indices], \" Best last validation accuracy :\", last_accuracies[last_indices])\n",
    "    print(\"Best max validation accuracy index :\", max_indices, \" Best last validation accuracy index :\", last_indices)\n",
    "\n",
    "    \n",
    "    return run_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "noshow"
    ]
   },
   "outputs": [],
   "source": [
    "## Test of parameter sweeps\n",
    "if False:\n",
    "    model_cls = TwoLayerTrainingParams\n",
    "    algorithm_cls = Algorithm.SGD\n",
    "\n",
    "    sweep_hidden_size = ps.plist('hidden_size', [20])\n",
    "    sweep_epochs = ps.plist('epochs', [5])\n",
    "    sweep_learning_rate = ps.plist('learning_rate', [1e-2])\n",
    "    sweep_learning_rate_decay = ps.plist('learning_rate_decay', [0.95])\n",
    "    sweep_regularization = ps.plist('regularization', [0.005])\n",
    "\n",
    "    df = run_campaign(\n",
    "            model_cls, algorithm_cls,\n",
    "            ps.pgrid(sweep_hidden_size, sweep_epochs, sweep_learning_rate, sweep_learning_rate_decay, sweep_regularization),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of layers (Two or Three)\n",
    "model_cls = ThreeLayerTrainingParams\n",
    "# Algorithm\n",
    "algorithm_cls = Algorithm.Adam\n",
    "\n",
    "# Sweep parameters\n",
    "sweep_hidden_size = ps.plist('hidden_size', [80, 100, 120])\n",
    "sweep_epochs = ps.plist('epochs', [30])\n",
    "sweep_learning_rate = ps.plist('learning_rate', [1e-2, 2e-2, 5e-2, (8e-2)])\n",
    "sweep_learning_rate_decay = ps.plist('learning_rate_decay', [0.95, 0.99])\n",
    "sweep_regularization = ps.plist('regularization', [0.005, 0.01, 0.02, (0.04)])\n",
    "\n",
    "# Store as a database file\n",
    "df = report_results_of(\n",
    "    run_campaign(\n",
    "        model_cls, algorithm_cls,\n",
    "        ps.pgrid(sweep_hidden_size, sweep_epochs, sweep_learning_rate, sweep_learning_rate_decay, sweep_regularization),\n",
    "    ),\n",
    "    model_cls, algorithm_cls\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "noshow"
    ]
   },
   "outputs": [],
   "source": [
    "# Number of layers (Two or Three)\n",
    "model_cls = TwoLayerTrainingParams\n",
    "# Algorithm\n",
    "algorithm_cls = Algorithm.Adam\n",
    "df = report_results_of(None, model_cls, algorithm_cls)\n",
    "df.iloc[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "noshow"
    ]
   },
   "outputs": [],
   "source": [
    "plot_stats(df.iloc[32]['stats_history'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "noshow"
    ]
   },
   "source": [
    "## Run on the test set\n",
    "When you are done experimenting, you should evaluate your final trained networks on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "noshow"
    ]
   },
   "outputs": [],
   "source": [
    "# best_2layer_sgd_prediction = None\n",
    "# best_3layer_sgd_prediction = None\n",
    "# best_2layer_adam_prediction = None\n",
    "# best_3layer_adam_prediction = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "noshow"
    ]
   },
   "source": [
    "## Kaggle output\n",
    "\n",
    "Once you are satisfied with your solution and test accuracy, output a file to submit your test set predictions to the Kaggle for Assignment 2 Neural Network. Use the following code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "noshow"
    ]
   },
   "outputs": [],
   "source": [
    "# output_submission_csv('./nn_2layer_sgd_submission.csv', best_2layer_sgd_prediction)\n",
    "# output_submission_csv('./nn_3layer_sgd_submission.csv', best_2layer_sgd_prediction)\n",
    "# output_submission_csv('./nn_2layer_adam_submission.csv', best_2layer_sgd_prediction)\n",
    "# output_submission_csv('./nn_3layer_adam_submission.csv', best_2layer_sgd_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare SGD and Adam\n",
    "Create graphs to compare training loss and validation accuracy between SGD and Adam. The code is similar to the above code, but instead of comparing train and validation, we are comparing SGD and Adam.\n",
    "\n",
    "\n",
    "We compare here the best parameter sets over our search space for SGD and Adam, for two and three layers respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison for two layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Layer SGD\n",
    "sgd_two_layer_optimal_params = {\n",
    "    \"hidden_size\": 120,\n",
    "    \"epochs\": 30,\n",
    "    \"learning_rate\": 8e-2,\n",
    "    \"learning_rate_decay\": 0.95,\n",
    "    \"regularization\": 0.01,\n",
    "}\n",
    "\n",
    "params = TwoLayerTrainingParams(**sgd_two_layer_optimal_params)\n",
    "trained_net_sgd_two_layer, stats_sgd_two_layer = train_model(params, Algorithm.SGD)\n",
    "\n",
    "\n",
    "adam_two_layer_optimal_params = {\n",
    "    \"hidden_size\": 100,\n",
    "    \"epochs\": 30,\n",
    "    \"learning_rate\": 1e-2,\n",
    "    \"learning_rate_decay\": 0.95,\n",
    "    \"regularization\": 0.005,\n",
    "}\n",
    "params = TwoLayerTrainingParams(**adam_two_layer_optimal_params)\n",
    "trained_net_adam_two_layer, stats_adam_two_layer = train_model(params, Algorithm.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "noshow"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_stats_for_comparison(sgd_stats, adam_stats, output = False):\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.io import write_image\n",
    "    from plotly.subplots import make_subplots\n",
    "\n",
    "    # fig = go.Figure()\n",
    "    fig = make_subplots(rows=3, \n",
    "                        cols=1, \n",
    "                        subplot_titles=(\"Weight norm history\", \"Loss history\", \"Accuracy history\"))\n",
    "    # l2 norm\n",
    "    fig.add_trace(go.Scatter(y=sgd_stats.weight_norms,\n",
    "                        mode='lines+markers', name=\"SGD W norm\"), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(y=adam_stats.weight_norms,\n",
    "                        mode='lines+markers', name=\"Adam W norm\"), row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"W norm\", row=1, col=1)\n",
    "\n",
    "    # losses\n",
    "    fig.add_trace(go.Scatter(y = sgd_stats.train_loss,\n",
    "                        mode='lines+markers',\n",
    "                        name='SGD training_loss'), row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(y = adam_stats.train_loss,\n",
    "                        mode='lines+markers',\n",
    "                        name='Adam training_loss'), row=2, col=1)\n",
    "    # Update xaxis properties\n",
    "    fig.update_yaxes(title_text=\"Loss\", row=2, col=1)\n",
    "\n",
    "    # losses\n",
    "    fig.add_trace(go.Scatter(y = sgd_stats.train_accuracy,\n",
    "                        mode='lines+markers',\n",
    "                        name='SGD train_accuracy'), row=3, col=1)\n",
    "    fig.add_trace(go.Scatter(y = sgd_stats.val_accuracy,\n",
    "                        mode='lines+markers',\n",
    "                        name='SGD val_accuracy'), row=3, col=1)\n",
    "    fig.add_trace(go.Scatter(y = adam_stats.train_accuracy,\n",
    "                        mode='lines+markers',\n",
    "                        name='Adam train_accuracy'), row=3, col=1)\n",
    "    fig.add_trace(go.Scatter(y = adam_stats.val_accuracy,\n",
    "                        mode='lines+markers',\n",
    "                        name='Adam val_accuracy'), row=3, col=1)\n",
    "    # Update xaxis properties\n",
    "    fig.update_yaxes(title_text=\"Accuracy\", row=3, col=1)\n",
    "    fig.update_xaxes(title_text=\"epochs\", row=3, col=1)\n",
    "\n",
    "    fig.show(renderer=\"notebook+pdf\")\n",
    "    if output:\n",
    "        write_image(fig, 'output_file.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats_for_comparison(stats_sgd_two_layer, stats_adam_two_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison for three layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_three_layer_optimal_params = {\n",
    "    \"hidden_size\": 120,\n",
    "    \"epochs\": 30,\n",
    "    \"learning_rate\": 5e-2,\n",
    "    \"learning_rate_decay\": 0.95,\n",
    "    \"regularization\": 0.01,\n",
    "}\n",
    "\n",
    "params = ThreeLayerTrainingParams(**sgd_three_layer_optimal_params)\n",
    "trained_net_sgd_three_layer, stats_sgd_three_layer = train_model(params, Algorithm.SGD)\n",
    "\n",
    "adam_three_layer_optimal_params = {\n",
    "    \"hidden_size\": 100,\n",
    "    \"epochs\": 30,\n",
    "    \"learning_rate\": 1e-2,\n",
    "    \"learning_rate_decay\": 0.95,\n",
    "    \"regularization\": 0.005,\n",
    "}\n",
    "\n",
    "# These are the best parameter sets found after running a fine search of the phase space\n",
    "params = ThreeLayerTrainingParams(**adam_three_layer_optimal_params)\n",
    "trained_net_adam_three_layer, stats_adam_three_layer = train_model(params, Algorithm.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats_for_comparison(stats_sgd_three_layer, stats_adam_three_layer)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Tejaswin Parthasarathy"
   }
  ],
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
